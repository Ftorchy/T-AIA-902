{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReplayMemory\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "#QLearning\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#DQN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size=10000, batch_size=64):\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"Ajoute une transition à la mémoire.\"\"\"\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Retourne un échantillon aléatoire de transitions.\"\"\"\n",
    "        if self.sufficient_memory():\n",
    "            return random.sample(self.memory, self.batch_size)\n",
    "        return random.sample(self.memory, self.__len__())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retourne le nombre de transitions stockées.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def sufficient_memory(self):\n",
    "        if self.__len__() > self.batch_size:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, 5)\n",
    "        self.fc1 = nn.Linear(5, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DeepQLearning:   \n",
    "    def __init__(self, learning_rate=0.8, gamma=0.95, exploration_prob=0.6, batch_size=64, target_net_update_freq=100, memory_size=10000):\n",
    "        self.set_learning_rate(learning_rate)\n",
    "        self.set_gamma(gamma)\n",
    "        self.set_exploration_prob(exploration_prob)\n",
    "        self.env = gym.make(\"Taxi-v3\")\n",
    "        \n",
    "        self.target_net_update_freq = target_net_update_freq\n",
    "        \n",
    "        self.max_memory_size= memory_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #Init la mémoire et les réseaux de neurones\n",
    "        self.init_memory()\n",
    "        self.init_net()\n",
    "\n",
    "        self.metrics = {\n",
    "            \"rewards\": [],\n",
    "            \"steps\": [],\n",
    "            \"success_rate\": [],\n",
    "            \"epochs\": 0,\n",
    "            \"training_time\": 0\n",
    "        }\n",
    "    \n",
    "    def init_net(self):\n",
    "        #Init les réseaux de neurones\n",
    "        input_dim = self.env.observation_space.n\n",
    "        output_dim = self.env.action_space.n\n",
    "        self.policy_net = DQN(input_dim, output_dim)\n",
    "        self.target_net = DQN(input_dim, output_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def init_memory(self):\n",
    "        #Init ReplayMemory\n",
    "        self.memory = ReplayMemory(self.max_memory_size, self.batch_size)\n",
    "\n",
    "    def train(self, epochs=10000):\n",
    "        if epochs <= 0:\n",
    "            raise self.__exception_factory(ValueError, \"The number of epochs cannot be 0 or negative !\")\n",
    "\n",
    "        self.metrics = {\n",
    "            \"rewards\": [],\n",
    "            \"steps\": [],\n",
    "            \"success_rate\": 0,\n",
    "            \"epochs\": epochs,\n",
    "            \"training_time\": 0\n",
    "        }\n",
    "        \n",
    "        #Réinitialise la mémoire et les réseaux de neurones pour l'entraînement\n",
    "        self.init_memory()\n",
    "        self.init_net()\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        for i in range(epochs):\n",
    "            self.__q_learning_algo(isTraining=True)\n",
    "            self.exploration_prob = max(0.6, self.exploration_prob - (1 / epochs))\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        self.metrics[\"training_time\"] = end_time - start_time\n",
    "        print(\"Entraînement terminé\")\n",
    "        print(\"Calcul des métriques en cours...\")\n",
    "        self.calculate_metrics()\n",
    "        print(\"Métriques calculées\")\n",
    "\n",
    "    def run(self):\n",
    "        self.env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "        self.__q_learning_algo()\n",
    "        #Redefine the environment to non human in case of futur training of the agent\n",
    "        self.env = gym.make(\"Taxi-v3\")\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        try:\n",
    "            with open(filename, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "                self.set_learning_rate(data[\"learning_rate\"])\n",
    "                self.set_gamma(data[\"gamma\"])    \n",
    "                self.set_exploration_prob(data[\"exploration_prob\"])\n",
    "\n",
    "                #Récupére le réseau de neurone\n",
    "                self.policy_net = data[\"policy_net\"]\n",
    "                self.target_net_update_freq = data[\"target_net_update_freq\"]\n",
    "                \n",
    "                #Récupére la mémoire\n",
    "                self.max_memory_size = data[\"memory_size\"]\n",
    "                self.batch_size = data[\"batch_size\"]\n",
    "                self.memory = data[\"memory\"]\n",
    "\n",
    "                self.metrics = data[\"metrics\"]\n",
    "        except:\n",
    "            print(\"An error occured while trying to open the file\") \n",
    "        \n",
    "\n",
    "    def save_model(self, filename=\"model\"):\n",
    "        try:\n",
    "            data = {\n",
    "                \"policy_net\": self.policy_net,\n",
    "                \"metrics\": self.metrics,\n",
    "                \"learning_rate\": self.learning_rate,\n",
    "                \"gamma\": self.gamma,\n",
    "                \"exploration_prob\": self.exploration_prob,\n",
    "                \"memory_size\": self.max_memory_size,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"memory\": self.memory,\n",
    "                \"target_net_update_freq\": self.target_net_update_freq\n",
    "            }\n",
    "            with open(filename + \".pickle\", \"wb\") as f:\n",
    "                pickle.dump(data, f)\n",
    "        except:\n",
    "            print(\"An error occured while trying to save the file\") \n",
    "\n",
    "    def __q_learning_algo(self, isTraining=False, isCalculate=False):\n",
    "            state = self.env.reset()\n",
    "            episode_over = False\n",
    "\n",
    "            run_reward = 0\n",
    "            step_count = 0\n",
    "            state = state[0]\n",
    "            success = False\n",
    "            \n",
    "            while not episode_over:\n",
    "                \n",
    "                #Epsilon policy\n",
    "                rand = np.random.rand()\n",
    "                if rand < self.exploration_prob and isTraining:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    state_result = torch.tensor([state], dtype=torch.long)\n",
    "                    q_values = self.policy_net(state_result)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "                \n",
    "                s_, reward, terminated, truncated, info = self.env.step(action)\n",
    "                \n",
    "                #Ajoute l'expérience à la mémoire\n",
    "                self.memory.push((state, action, reward, s_, terminated))\n",
    "                \n",
    "                episode_over = terminated or truncated\n",
    "                success = terminated\n",
    "                state = s_\n",
    "\n",
    "                if isTraining: \n",
    "                    self.optimize_model()\n",
    "\n",
    "                    # Update target network periodically\n",
    "                    if step_count % self.target_net_update_freq == 0:\n",
    "                        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "                    run_reward += reward\n",
    "                step_count += 1\n",
    "                    \n",
    "\n",
    "            if isCalculate: \n",
    "                if run_reward != 0:\n",
    "                    self.metrics[\"rewards\"].append(run_reward / step_count)\n",
    "                else:\n",
    "                    self.metrics[\"rewards\"].append(0)\n",
    "                self.metrics[\"steps\"].append(step_count)\n",
    "                if success:\n",
    "                    self.metrics[\"success_rate\"] += 1\n",
    "\n",
    "            self.env.close()\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if self.memory.sufficient_memory() == False:\n",
    "            return \n",
    "        \n",
    "        batch = self.memory.sample()\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "\n",
    "        state_batch = torch.IntTensor(state_batch)\n",
    "        action_batch = torch.LongTensor(action_batch).unsqueeze(1)\n",
    "        reward_batch = torch.IntTensor(reward_batch)\n",
    "        next_state_batch = torch.IntTensor(next_state_batch)\n",
    "        done_batch = torch.IntTensor(done_batch)\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.policy_net(state_batch).gather(1, action_batch).squeeze()\n",
    "\n",
    "        # Compute target Q-values using the target network\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = self.target_net(next_state_batch).max(1)[0]\n",
    "            target_q_values = reward_batch + self.gamma * max_next_q_values * (1 - done_batch)\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        for i in range(1000):\n",
    "            self.__q_learning_algo(isCalculate=True)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return self.metrics\n",
    "\n",
    "    def show_metrics(self):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(200, 5))\n",
    "        # axs[0].plot(rewards, 'tab:green')\n",
    "        # axs[0].set_title(\"Reward\")\n",
    "        axs[0].plot(self.metrics[\"steps\"], 'tab:purple')\n",
    "        axs[0].set_title(\"Step Count\")\n",
    "\n",
    "        print(\"Overall Average reward:\", np.mean(self.metrics[\"rewards\"]))\n",
    "        print(\"Overall Average number of steps:\", np.mean(self.metrics[\"steps\"]))\n",
    "        print(\"Success rate (%):\", self.metrics[\"success_rate\"] / 1000 * 100)\n",
    "        print(\"Number of epochs:\", self.metrics[\"epochs\"])\n",
    "        print(\"Training Time(in secondes):\", self.metrics[\"training_time\"])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def set_learning_rate(self, lr):\n",
    "        if self.__check_is_between_0_and_1(value=lr, name=\"learning rate\"):\n",
    "            self.learning_rate = lr\n",
    "\n",
    "    def set_gamma(self, gamma):\n",
    "        if self.__check_is_between_0_and_1(value=gamma, name=\"gamma\"):\n",
    "            self.gamma = gamma\n",
    "\n",
    "    def set_exploration_prob(self, exploration_prob):\n",
    "        if self.__check_is_between_0_and_1(value=exploration_prob, name=\"exploration_prob\"):\n",
    "            self.exploration_prob = exploration_prob\n",
    "\n",
    "    def __check_is_between_0_and_1(self, value, name):\n",
    "        message = f\"The {name} hyperparameter must be between 0 and 1! \\n\"\n",
    "        if value > 1:\n",
    "            message += \"Actually he is superior to 1!\"\n",
    "            raise self.__exception_factory(ValueError, message)\n",
    "        elif value <= 0:\n",
    "            message += \"He cannot be null or negatif!\"\n",
    "            raise self.__exception_factory(ValueError, message)\n",
    "        return True\n",
    "            \n",
    "    def __exception_factory(self, exception, message):\n",
    "        return exception(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DeepQLearning(learning_rate=0.001, gamma=0.95, exploration_prob=0.99, memory_size=2000, target_net_update_freq=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.show_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model(\"my_model_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = DeepQLearning()\n",
    "agent2.load_model(\"my_model_dqn.pickle\")\n",
    "agent2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's done to test the raising exception when we pass a bad parameter\n",
    "agent3 = QLearning(learning_rate=1.5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
